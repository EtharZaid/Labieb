
import os
from openai import OpenAI
from pdf2image import convert_from_bytes


client = OpenAI(

    api_key='Insert your API Key here'
)


#Function needed to encode the images into a proper format for the GPT request
def encode_image(image):
    buffered = io.BytesIO()
    image.save(buffered, format="jpeg")
    return base64.b64encode(buffered.getvalue()).decode('utf-8')


#Function to convert PDF report into text using GPT-4o
def ConvertPDFtoText(pages):

    base64_image=[]
    for image in pages:
        base64_image.append(encode_image(image))    

    api_key = 'Insert your API Key here'
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    response = client.chat.completions.create(
        model="gpt-4o",
        max_tokens= 4096,
        messages=[
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "text": "Please convert the content of the pathology report to text. You might be given multiple images, they should be converted into one text response. Your response should not include anything else. If the report exceeds the maximum token limit, indicate at the end with the statement 'Response truncated due to token limit.'"
              }
              ] + [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{img}"
                        }
                    } for img in base64_image
                ]
            }
        ])
        
    
    return response.choices[0].message.content


#Read PDF report and convert it to text
pdf_path='path/to/report.PDF'
pages = convert_from_path(pdf_path, fmt='jpeg')
TextReport=ConvertPDFtoText(pages)

#The path to the prompt text file
path_to_prompt='path/to/prompt.txt'

#Read the file
with open(path_to_prompt, 'r') as text_prompt: 
	Prompt = text_prompt.read()

#Merge the prompt with the report
Prompt=Prompt%TextReport


#Prepare the GPT request
message=[{"role": "user", "content":Prompt}]


# Generate the answer using OpenAI API
# You may adjust the parameters as you see fit
response = client.chat.completions.create(

    model="gpt-4o", #model name
    messages=message,
    max_tokens=70,  #The maximum number of tokens to be generated by the model
    temperature=0.2, #Determines 'How chatty' the model should be, smaller values yealds more precise responses 
    n=1, #The number of responses generated by the model per request
    stop=None #Used to make the model stop generating tokens at a desired point.
)

#Print out the response generated by the model
print("The model responded \n:",response.choices[0].message.content)        

